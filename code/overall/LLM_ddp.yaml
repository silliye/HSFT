# general
seed: 2020
state: INFO
use_text: True
reproducibility: True
checkpoint_dir: 'saved'
show_progress: True

log_wandb: False
wandb_project: 'REC'
MAX_ITEM_LIST_LENGTH: 50
MAX_TEXT_LENGTH: 64

data_path: ./dataset/                       # dataset path
dataset: Pixel200K                        # dataset name
test_dataset: Pixel200K                        # dataset name

text_path: text_path # Use absolute path
text_keys: ['title', 'tag', 'description']

item_prompt: 'Compress the following sentence into embedding: '
item_emb_token_n: 1

# training settings
epochs: 5
train_batch_size: 64
optim_args: {
  learning_rate: 1e-4,
  weight_decay: 0.01
}
scheduler_args: {
  type: cosine,
  warmup: 0.1
}

# eval settings
eval_batch_size: 16
topk: [5,10]
metrics: ['Recall', 'NDCG']
valid_metric: NDCG@10
metric_decimal_place: 7
eval_step: 1 
stopping_step: 5

strategy: ddp
precision: 32-true

use_qlora: True              # 总开关：设为 true 来启用下面的QLoRA微调
qlora_r: 8                   # LoRA的秩
qlora_alpha: 16              # LoRA的alpha
qlora_dropout: 0.05          # LoRA层的dropout率
qlora_target_modules: ['q_proj', 'v_proj'] # 针对Llama模型，在Q和V矩阵上应用LoRA